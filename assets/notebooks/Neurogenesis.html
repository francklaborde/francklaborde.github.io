<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Neurogenesis</title>
  <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown">
<div style="font-size:30pt; line-height:25pt; font-weight:bold; text-align:center;">When, where and how to add new neurons to ANNs </div>
</div>
<section id="introduction" class="cell markdown">
<h1>Introduction</h1>
</section>
<div class="cell markdown">
<p>Neural networks have come a long way, especially deep models with
lots of neurons. Most of these models have a fixed structure, meaning we
decide on the number of neurons in each layer before training and then
tweak the parameters based on the dataset.</p>
<p>However, there's a new approach in deep learning that's all about
<strong>dynamic networks</strong>. Unlike the fixed ones, dynamic
networks not only learn parameters during training but can also change
their structure as they learn.</p>
<p>This shift to dynamic networks has some advantages. It makes training
more efficient and eliminates the need for manual tweaking of
architectures. Dynamic networks can adapt to new information and tasks
easily. They can even add or remove parts like neurons and connections
as needed.</p>
<p>This notebook is about understanding dynamic networks, focusing on a
specific idea called <strong>structural pruning</strong>. This process
involves selectively removing connections or neurons, showing a bit of a
concept called neurogenesis. Neurogenesis is like adding new elements
within existing layers. Structural pruning, as you'll see in this
notebook, is an example that highlights how dynamic networks can keep
evolving and improving.</p>
</div>
<section id="artificial-neural-networks-anns" class="cell markdown">
<h1>Artificial Neural Networks (ANNs)</h1>
</section>
<div class="cell markdown">
<p>First, let's make a short reminder about Artificial Neural
Networks.</p>
<p>Artificial Neural Networks (ANNs) represent a class of computational
models inspired by the neural architecture of the human brain. Operating
as interconnected nodes organized into layers, ANNs leverage a
combination of linear and non-linear transformations to capture complex
relationships within data.</p>
<p>At their core, ANNs consist of an input layer, one or more hidden
layers, and an output layer. Each connection between nodes is associated
with a weight, and the network learns by adjusting these weights during
a training phase. The training process involves minimizing a predefined
loss function, typically through optimization algorithms like stochastic
gradient descent (SGD).</p>
<p>Activation functions introduce non-linearities into the network,
enabling the modeling of intricate mappings between inputs and outputs.
Common activation functions include Rectified Linear Units (ReLU),
Sigmoid, and Hyperbolic Tangent (tanh), each influencing the network's
capacity to capture and propagate information through the layers..</p>
<p>Training large-scale ANNs requires careful consideration of
hyperparameters, such as learning rates, batch sizes, and regularization
techniques to prevent overfitting.</p>
<p>You can find more information and detail on <strong><a
href="https://github.com/SupaeroDataScience/deep-learning">this
github</a></strong></p>
<center><img src="img\ann.png" width="600px"></img></center>
</div>
<section id="time-to-code-" class="cell markdown">
<h2>Time to code !!</h2>
<p>Let's make a little ANN to classify the <strong><a
href="https://en.wikipedia.org/wiki/MNIST_database">MNIST
dataset</a></strong>. The MNIST dataset is composed of a collection of
handwritten digit images, MNIST stands as a foundational resource for
training and evaluating algorithms designed to recognize and classify
digits from 0 to 9. Each image in the MNIST dataset is a grayscale,
28x28-pixel representation of a handwritten digit.</p>
<p><strong>We will use Pytorch but you can do the same thing with
Keras</strong></p>
<p>First, let's load the data and print 9 random figures :</p>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.sampler <span class="im">import</span> SubsetRandomSampler</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We need a, image transform to convert to Tensor and normalize with a mean of 0.5 and standard deviation of 0.5</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the Train and Test set</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> datasets.MNIST(<span class="st">&#39;.&#39;</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> datasets.MNIST(<span class="st">&#39;.&#39;</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a 3x3 grid for displaying the images</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get 9 random indices from the training set</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>random_indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(trainset), size<span class="op">=</span><span class="dv">9</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the 9 random images</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(random_indices):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    image, label <span class="op">=</span> trainset[idx]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.numpy().squeeze()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    axes[i].imshow(image, cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f&quot;Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="img/MNIST.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the dataloader</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>valid_size <span class="op">=</span> <span class="fl">0.2</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the training set into training and validation sets</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> <span class="bu">len</span>(trainset)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(num_train))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(indices)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="bu">int</span>(np.floor(valid_size <span class="op">*</span> num_train))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>train_idx, valid_idx <span class="op">=</span> indices[split:], indices[:split]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the samplers for obtaining training and validation batches</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>train_sampler <span class="op">=</span> SubsetRandomSampler(train_idx)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>valid_sampler <span class="op">=</span> SubsetRandomSampler(valid_idx)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the dataloaders</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(trainset, batch_size<span class="op">=</span>batch_size, sampler<span class="op">=</span>train_sampler)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> DataLoader(trainset, batch_size<span class="op">=</span>batch_size, sampler<span class="op">=</span>valid_sampler)</span></code></pre></div>
</div>
<div class="cell markdown">
<p>Now that we have prepared the data, we can implement a little ANN,
with 2 hidden layers to classify the figures.</p>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>nb_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="36">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model architecture of the neural network fully connected</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ANN(nn.Module):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, hidden_layer_1, hidden_layer_2):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> nn.Sequential(</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_size, hidden_layer_1, bias<span class="op">=</span><span class="va">False</span>), </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_layer_1, hidden_layer_2, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_layer_2, output_size, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            nn.LogSoftmax(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(x)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> <span class="dv">28</span><span class="op">*</span><span class="dv">28</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>hidden_layer_1 <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>hidden_layer_2 <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>model_ann <span class="op">=</span> ANN(input_size, output_size, hidden_layer_1, hidden_layer_2)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the loss function and the optimizer</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.NLLLoss()</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model_ann.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_ann)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>ANN(
  (network): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=False)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=False)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=False)
    (5): LogSoftmax(dim=1)
  )
)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="37">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(nb_epochs),desc<span class="op">=</span><span class="st">&quot;Training of the ANN&quot;</span>):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    model_ann.train()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> train_loader:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model_ann(images)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, labels)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(loss)</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    list_loss.append(running_loss<span class="op">/</span><span class="bu">len</span>(train_loader))</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f&quot;Epoch nb {i+1}. Training loss: {running_loss/len(train_loader)}&quot;)</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>Training of the ANN:   0%|          | 0/10 [00:00&lt;?, ?it/s]</code></pre>
</div>
<div class="output stream stderr">
<pre><code>Training of the ANN: 100%|██████████| 10/10 [05:41&lt;00:00, 34.12s/it]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="38">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training loss</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plt.plot(list_loss)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epochs&#39;</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Training loss&#39;</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training loss&#39;</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="img/Training_loss.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="39">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the model</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>model_ann.<span class="bu">eval</span>()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> valid_loader:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model_ann(images)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(output, <span class="dv">1</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>torch.save(model_ann.state_dict(), <span class="st">&#39;model.pt&#39;</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Accuracy of the model on the validation set: </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total<span class="sc">:.2f}</span><span class="ss">%&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy of the model on the validation set: 97.51%
</code></pre>
</div>
</div>
<section id="structural-pruning" class="cell markdown">
<h1>Structural pruning</h1>
</section>
<div class="cell markdown">
<p>Now that we've established our neural network, consisting of two
layers with 512 neurons each, we can explore techniques to prune it
effectively. Pruning aims to streamline the network, reducing training
and validation time without compromising performance. Among various
pruning methods, two stand out as the most commonly used:</p>
<ol>
<li>Weight pruning</li>
<li>Neuron pruning</li>
</ol>
<p>Weight pruning involves removing connections between neurons by
setting the corresponding weights to zero. Conversely, neuron pruning
entails eliminating entire neurons by zeroing out all connections
associated with the neuron being pruned. The distinction between these
methods is illustrated in the image below.</p>
<center><img src="img\Pruning image.webp" width="500px"></img></center>
<p>We are going to see how weight pruning works</p>
</div>
<section id="weight-pruning" class="cell markdown">
<h2>Weight pruning</h2>
</section>
<div class="cell markdown">
<p>In the technique of weight pruning, we selectively remove connections
within a neural network by setting individual weights in the weight
matrix to zero. This process effectively sparsifies the network,
reducing its complexity and potentially improving its efficiency during
both training and inference stages.</p>
<p>The mechanism of weight pruning involves ranking the individual
weights in the weight matrix <span class="math inline"><em>W</em></span>
based on their absolute value. By doing so, we obtain a sorted list of
weights.</p>
<p>To achieve a desired level of sparsity, denoted as <span
class="math inline"><em>k</em>%</span>, we then proceed to set to zero
the smallest <span class="math inline"><em>k</em>%</span> of weights
from the sorted list. This corresponds to deleting a certain percentage
of connections in the neural network, effectively reducing the number of
parameters and connections within the model.</p>
<p>The degree of sparsity, represented by <span
class="math inline"><em>k</em>%</span>, can be adjusted based on
specific requirements or constraints. Higher values of <span
class="math inline"><em>k</em>%</span> result in more aggressive
pruning, leading to greater sparsity but potentially at the expense of
model performance. Conversely, lower values of <span
class="math inline"><em>k</em>%</span> preserve more connections and
parameters, thus retaining more of the network's original capacity.</p>
</div>
<div class="cell markdown">
<p>Now that we know how the weight pruning works, we can then write a
pseudo code to implement it in Python :</p>
<pre><code>for l in layers(model):
    Compute absolute values of all weights in W.
    Sort weights in ascending order based on their absolute values.
    Set to zero the smallest k% of weights.
    Replace the weight by the pruned weights in the matrix W.</code></pre>
</div>
<div class="cell markdown">
<p>We will rewrite the code implemented in Pytorch to understand how it
works. We will use the useful fonctions and the skeleton class that you
can find <a
href="https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/prune.py">here</a>.</p>
</div>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Skeleton of the pruning class</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.prune <span class="im">import</span> BasePruningMethod</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Some useful fonction to validate the pruning amount </span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If you want to see how they work, you can look at the link right above to have the full code</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.prune <span class="im">import</span> _validate_pruning_amount_init, _validate_pruning_amount</span></code></pre></div>
</div>
<div class="cell markdown">
<p>To find the weights that need to be set at 0, we write a fonction
that return the indices of the k first elements based on their
values.</p>
</div>
<div class="cell code" data-execution_count="41">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weights_to_shut(input_tensor, k):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten the input tensor</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    flattened_input <span class="op">=</span> input_tensor.flatten()</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use torch.sort to get the indices of the top k elements</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    sorted_indices <span class="op">=</span> torch.argsort(flattened_input)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take the top k indices</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    topk_indices <span class="op">=</span> sorted_indices[:k]</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> topk_indices</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="42">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Weight_Pruning(BasePruningMethod):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">        amount (float): quantity of parameters to prune.</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">            It should be between 0.0 and 1.0 and represent the</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">            fraction of parameters to prune.</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    PRUNING_TYPE <span class="op">=</span> <span class="st">&quot;unstructured&quot;</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, amount):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check range of validity of pruning amount</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        _validate_pruning_amount_init(amount)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.amount <span class="op">=</span> amount</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_mask(<span class="va">self</span>, t, default_mask):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># t is the tensor to prune, meaning the weights of the layer</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check that the amount of units to prune is not &gt; than the number of</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># parameters in t</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        tensor_size <span class="op">=</span> t.nelement()</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute number of units to prune: amount * tensor_size</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        nparams_toprune <span class="op">=</span> <span class="bu">round</span>(<span class="va">self</span>.amount <span class="op">*</span> tensor_size)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This should raise an error if the number of units to prune is larger</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># than the number of units in the tensor</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        _validate_pruning_amount(nparams_toprune, tensor_size)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create mask containing only ones</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> default_mask.clone(memory_format<span class="op">=</span>torch.contiguous_format)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> nparams_toprune <span class="op">!=</span> <span class="dv">0</span>:  <span class="co"># k=0 not supported by torch.kthvalue            </span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the indices to zero out</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>            indices_at_0 <span class="op">=</span> weights_to_shut(torch.<span class="bu">abs</span>(t).view(<span class="op">-</span><span class="dv">1</span>), k<span class="op">=</span>nparams_toprune)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Set the elements to zero</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>            mask.view(<span class="op">-</span><span class="dv">1</span>)[indices_at_0] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mask</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>    <span class="at">@classmethod</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">apply</span>(cls, module, name, amount, importance_scores<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">r&quot;&quot;&quot;Add pruning on the fly and reparametrization of a tensor.</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Adds the forward pre-hook that enables pruning on the fly and</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a><span class="co">        the reparametrization of a tensor in terms of the original tensor</span></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co">        and the pruning mask.</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a><span class="co">            module (nn.Module): module containing the tensor to prune</span></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="co">            name (str): parameter name within ``module`` on which pruning</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="co">                will act.</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a><span class="co">            amount (int or float): quantity of parameters to prune.</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="co">                It should be between 0.0 and 1.0 and represent the</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a><span class="co">                fraction of parameters to prune.</span></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a><span class="co">            importance_scores (torch.Tensor): tensor of importance scores (of same</span></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="co">                shape as module parameter) used to compute mask for pruning.</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="co">                The values in this tensor indicate the importance of the corresponding</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a><span class="co">                elements in the parameter being pruned.</span></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="co">                If unspecified or None, the module parameter will be used in its place.</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().<span class="bu">apply</span>(</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>            module, name, amount<span class="op">=</span>amount, importance_scores<span class="op">=</span>importance_scores</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="44">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.utils.prune <span class="im">as</span> prune</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>list_accuracy_weight <span class="op">=</span> []</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">20</span>)):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ANN(input_size, output_size, hidden_layer_1, hidden_layer_2)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(<span class="st">&#39;model.pt&#39;</span>))</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    prune_l1 <span class="op">=</span> Weight_Pruning(amount<span class="op">=</span>i<span class="op">/</span><span class="dv">20</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, module <span class="kw">in</span> model.named_modules():</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            prune_l1.<span class="bu">apply</span>(module, name<span class="op">=</span><span class="st">&#39;weight&#39;</span>, amount<span class="op">=</span>i<span class="op">/</span><span class="dv">20</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test the model</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> valid_loader:</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(images)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(output, <span class="dv">1</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    list_accuracy_weight.append(<span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 20/20 [01:04&lt;00:00,  3.24s/it]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let's plot the accuracy of the validation set against the sparsity to
see how removing connections impact the performance of our neural
network</p>
</div>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the accuracy</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">/</span><span class="dv">20</span> <span class="op">*</span> np.arange(<span class="dv">20</span>), list_accuracy_weight)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Pruning level&#39;</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Accuracy&#39;</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Accuracy vs Pruning level&#39;</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="img/Accuracy_pruning.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>As observed, the act of pruning connections has a negligible impact
on the model's performance until approximately 80% sparsity is achieved,
indicating the removal of a significant portion of connections. This
technique affords us the opportunity to substantially enhance
performance, showcasing its efficacy in optimizing the model's
efficiency without compromising its effectiveness. So we could change
our model to a one with only 80% of neurons it will be barely the same
performance.</p>
</div>
<section id="neurogenesis" class="cell markdown">
<h1>Neurogenesis</h1>
</section>
<div class="cell markdown">
<p>Now that we have seen how a dynamic neural network works, we can
better understand how the neurogenesis works. We consider neurogenesis
through the following dimensions about adding neurons:</p>
<ul>
<li><strong>When</strong>: Standard learning algorithms naturally
discretize the learning process into steps, so neurogenesis may occur at
any step of training.</li>
<li><strong>How many</strong>: When neurogenesis is triggered at a step,
multiple neurons can be added at once.</li>
<li><strong>Where</strong>: In standard ANN architectures, this amounts
to which layer neurogenesis occurs in.</li>
<li><strong>How</strong>: The fan-in and fan-out weights and bias of
each new neuron must be initialized.</li>
</ul>
<p>Existing neurogenesis research primarily focuses on
<strong>when</strong> and <strong>how</strong> to initialize new
neurons. Some studies delay neurogenesis until later in training, while
others advocate for its occurrence throughout training. Recent
approaches often use gradient-based methods to initialize new neurons,
either by splitting existing neurons or creating entirely new ones based
on gradient norms. Neurogenesis expands the parameter search space and
can either preserve the network's function or alter it. To unify these
approaches and study their components, we will see an a framework for
neurogenesis strategies.</p>
</div>
<section id="framework" class="cell markdown">
<h2>Framework</h2>
</section>
<div class="cell markdown">
<p>The neurogenesis framework divides neurogenesis strategies into
triggers, which are heuristics that determine when, where, and how many
neurons to add, and initializations, which determine how to set their
weights before training them. We present the basic framework below.
After every gradient step, for each layer, the trigger is evaluated to
determine if and how many neurons to add, and then the initialization is
used to add these new neurons. Since the trigger is assessed after each
gradient step, the decision of when to add neurons is based on non-zero
outputs from the trigger. Similarly, the decision of where to add
neurons is addressed by evaluating the trigger on each layer
independently.</p>
</div>
<div class="cell markdown">
<hr />
<p>Neurogenesis framework</p>
<pre class="{python}"><code>Procedure Neurogenesis(Trigger, Initialization, initial ANN 𝑓)
    while 𝑓 not converged do
        Gradient descent step on current existing weights
        for each hidden layer 𝑙 do
            if Trigger(𝑓,𝑙) &gt; 0 then
                Add Trigger(𝑓,𝑙) neurons using Initialization(𝑓,𝑙)
    return trained 𝑓</code></pre>
<hr />
</div>
<section id="triggers" class="cell markdown">
<h3>Triggers</h3>
</section>
<div class="cell markdown">
<p>We will see three sources of information for trigger for neurogenesis
: neural activations, weights, and gradients. These triggers determine
if and how many neurons to add for each layer after each gradient
step.</p>
</div>
<div class="cell markdown">
<ol>
<li><p><strong>Activation based</strong> : When constructing an
efficient network via neurogenesis, the aim is to add neurons that
introduce novel features to improve the direction of descent, reduce
empirical risk, or avoid redundancy. Thus, we need to measure how
different or orthogonal the post-activations are from each other. To
measure orthogonality of a layer, we use the 𝜖-numerical rank of the
post-activation matrix. For layer 𝑙 and 𝑛 samples generating the
post-activation <span
class="math inline"><em>H</em><sub><em>l</em></sub></span> the effective
dimension metric may be estimated by: <span class="math display">$$
\phi_a^{ED} (f,l) = \frac{1}{M_l} \bigg|\bigg\{\sigma \in
\text{SVD}\bigg(\frac{1}{\sqrt{n}} H_l\bigg)\bigg| \sigma &gt; \epsilon
\bigg\}\bigg| $$</span> Where <span
class="math inline">$\text{SVD}\bigg(\frac{1}{\sqrt{n}}
H_l\bigg)$</span> is the set of singular value of <span
class="math inline">$\frac{1}{\sqrt{n}} H_l$</span>, <span
class="math inline"><em>ϵ</em> &gt; 0</span> is a small threshold and
<span class="math inline"><em>M</em><sub><em>l</em></sub></span> is the
number of rows of the matrix <span
class="math inline"><em>W</em><sub><em>l</em></sub></span>, equivalent
to the number of entries of the layer <span
class="math inline"><em>l</em></span>. This metric estimates the
effective dimension of post-activation matrices, indicating potential
for adding new neurons when orthogonality is high and redundancy when
it's low. The addition of a new neuron increases the metric if its
activation is more orthogonal to others and decreases it if it's
redundant. The baseline metric value at network initialization guides
the process, aiming to maintain or increase it throughout training. The
number of neurons triggered is based on the difference between the
current metric value and a threshold, scaled by the current number of
neurons.
Now that we have a metric the goal is to preserve this initial metric
value throughout training, even as the layer expands. If new neurons
fail to increase the rank and decrease the metric value, neurogenesis
pauses until the rank improves through gradient descent. This baseline
value is multiplied by a threshold hyperparameter <span
class="math inline"><em>γ</em><sub><em>a</em></sub></span> close to 1.
The number of triggered neurons is then : <span
class="math display"><em>T</em><sub><em>a</em><em>c</em><em>t</em></sub>(<em>f</em>,<em>ϕ</em><sub><em>a</em></sub>,<em>l</em>) = min (0,⌊<em>M</em><sub><em>l</em></sub>(<em>ϕ</em><sub><em>a</em></sub>(<em>f</em>,<em>l</em>)−<em>γ</em><sub><em>a</em></sub><em>ϕ</em><sub><em>a</em></sub>(<em>f</em><sub>0</sub>,<em>l</em>))⌋)</span>
where <span class="math inline"><em>f</em><sub>0</sub></span> is the ANN
at initialization and <span
class="math inline"><em>ϕ</em><sub><em>a</em></sub></span> is the
orthogonality metric.</p></li>
<li><p><strong>Weight based</strong> : We include weight matrix
orthogonality as a comparison to activation-based methods. The trigger,
<span
class="math inline"><em>T</em><sub><em>w</em></sub><em>e</em><em>i</em><em>g</em><em>h</em><em>t</em></span>
, is computed as in the two equations in the activation based but with
<span class="math inline"><em>W</em><sub><em>l</em></sub></span> instead
of <span class="math inline"><em>H</em><sub><em>l</em></sub></span> :
<span class="math display">$$\phi_w^{ED} (f,l) = \frac{1}{M_l}
\bigg|\bigg\{\sigma \in \text{SVD}\bigg(\frac{1}{\sqrt{n}}
W_l\bigg)\bigg| \sigma &gt; \epsilon \bigg\}\bigg|$$</span> <span
class="math display"><em>T</em><sub><em>w</em><em>e</em><em>i</em><em>g</em><em>h</em><em>t</em></sub>(<em>f</em>,<em>ϕ</em><sub><em>w</em></sub>,<em>l</em>) = min (0,⌊<em>M</em><sub><em>l</em></sub>(<em>ϕ</em><sub><em>w</em></sub>(<em>f</em>,<em>l</em>)−<em>γ</em><sub><em>w</em></sub><em>ϕ</em><sub><em>w</em></sub>(<em>f</em><sub>0</sub>,<em>l</em>))⌋)</span></p></li>
<li><p><strong>Gradient based</strong> : To investigate gradient-based
neurogenesis initializations within dynamic neurogenesis, we will use a
trigger based on the auxiliary gradient. This trigger assesses the
maximum increase in gradient norm attainable by adding <span
class="math inline"><em>k</em></span> neurons to the <span
class="math inline"><em>l</em>th</span> layer as the sum of the largest
<span class="math inline"><em>k</em></span> singular values of the
auxiliary gradient matrix <span class="math inline">$\frac{\partial
L}{\partial z_{l-1}} h_{l-1}$</span>. By comparing these singular values
with the gradient norms of all existing neurons in that layer over the
same dataset, we determine the number of neurons to trigger based on the
count of singular values exceeding the sum of gradient norms. <span
class="math display">$$ \phi_{grad} (f, L, l) = \frac{1}{M_l}
\bigg|\bigg\{\sigma \in
\text{SVD}\bigg(\frac{\partial{L}}{\partial{Z_{l+1}}}
H_{l-1}^{\text{T}}\bigg)\bigg| \sigma &gt; \sum_{m=1}^{M_l} \bigg|
\bigg| \frac{\partial{L}}{\partial{w_{m}^{in}}}\bigg| \bigg|_F + \bigg|
\bigg| \frac{\partial{L}}{\partial{w_{m}^{out}}}\bigg| \bigg|_F
\bigg\}\bigg| $$</span> where <span
class="math inline"><em>L</em></span> is the loss function if the
network, <span
class="math inline"><em>w</em><sub><em>m</em></sub><sup><em>i</em><em>n</em></sup></span>
is the <span
class="math inline"><em>m</em><sup><em>t</em><em>h</em></sup></span> row
of <span class="math inline"><em>W</em><sub><em>l</em></sub></span> and
<span
class="math inline"><em>w</em><sub><em>m</em></sub><sup><em>o</em><em>u</em><em>t</em></sup></span>
is the <span
class="math inline"><em>m</em><sup><em>t</em><em>h</em></sup></span>
column of <span
class="math inline"><em>W</em><sub><em>l</em> + 1</sub></span>,
respectively representing the fan-in and fan_out weight of the <span
class="math inline"><em>m</em><sup><em>t</em><em>h</em></sup></span>
neuron of the layer <span class="math inline"><em>l</em></span>. We
intuit this threshold creates neurons that could have initial gradients
significantly stronger than existing neurons and will be harder to
surpass as the layer grows, thus leading to convergence in layer
width.</p></li>
</ol>
</div>
<section id="initializations" class="cell markdown">
<h3>Initializations</h3>
</section>
<div class="cell markdown">
<p>For the initialization of new neurons, we aim to minimize
computational overhead by leveraging information already computed for
the corresponding trigger in each method. Then, we introduce
function-preserving neurons that do not immediately alter the network
output or the activation of downstream layers. Our proposal suggests
that the role of initialization during training neurogenesis is to add
locally useful neurons to the triggered layer, which are subsequently
integrated into the network's functionality via gradient descent. We
assess the utility of these neurons based on activation, weights, and
gradients. Additionally, we standardize the weight norm of new neurons
to match that of existing neurons, simplifying subsequent
calculations.</p>
<p>Note : The suite of neurogenesis strategies all begin with NORTH,
that means Neural ORTHogonality for the initialization</p>
</div>
<div class="cell markdown">
<ol>
<li><p><strong>Activation based</strong> : We present multiple
approaches for initializing neurons to achieve orthogonal
post-activations. However, due to the nonlinearity of neurons,
closed-form solutions are only available for a set of fan-in weights,
resulting in orthogonal pre-activations rather than post-activations.
These approaches generate candidate neurons and select those that
independently maximize the orthogonality metric of the post-activations
with existing neurons in the layer. The fan-out weights are initialized
to 0.</p>
<ul>
<li><strong><em>NORTH-Select</em></strong> : generate random candidate
neurons.</li>
<li><strong><em>NORTH-Pre</em></strong> : generate candidate neurons
with pre-activations maximally orthogonal to existing pre-activations.
For <span class="math inline"><em>n</em></span> samples and <span
class="math inline"><em>M</em><sub><em>l</em></sub></span> neurons in
layer <span class="math inline"><em>l</em></span> , a candidate’s fan-in
weights are <span
class="math display"><strong>w</strong> = (<strong>H</strong><sub><em>l</em> − 1</sub><sup>⊺</sup>)<sup>−1</sup><strong>V</strong><sub><em>Z</em><sub><em>l</em></sub></sub><sup>′</sup><strong>a</strong><sup>⊺</sup></span>
where <span
class="math inline">(<strong>H</strong><sub><em>l</em> − 1</sub><sup>⊺</sup>)<sup>−1</sup></span>
is the left inverse of the transpose of post-activations <span
class="math inline"><em>H</em><sub><em>l</em> − 1</sub></span> for layer
<span class="math inline"><em>l</em> − 1</span>, <span
class="math inline"><em>V</em><sub><em>Z</em><sub><em>l</em></sub></sub><sup>′</sup></span>
is comprised of orthogonal vectors of the kernel of pre-activations
<span class="math inline"><em>Z</em><sub><em>l</em></sub></span>, and
<span
class="math inline"><em>a</em> ∈ ℝ<sup><em>n</em> − <em>M</em><sub><em>l</em></sub></sup></span>
is a random vector resampled for each candidate.
As a baseline to these selection-based techniques,
<strong><em>NORTH-Random</em></strong> uses RandomInit as the
initialization strategy with random fan-in weights and zeroed fan-out
weights.</li>
</ul></li>
<li><p><strong>Weight based</strong> : The objective is to initialize
<span class="math inline"><em>k</em></span> new neurons with fan-out
weights set to 0, while the fan-in weights are designed to be orthogonal
to the existing weights in the layer. So, each new neuron is initialized
with fan-out weights of 0 and fan-in weights determined by projecting
random initial weights onto the kernel (null space) of the existing
weight matrix <span
class="math inline"><em>W</em><sub><em>l</em></sub></span> : <span
class="math display"><strong>w</strong> = proj<sub>ker <em>W</em><sub><em>l</em></sub></sub>(<strong>w</strong><sub><em>i</em></sub>)</span>
where $ \textbf{w}<em>i \in \mathbb{R}^{M</em>{l-1} + 1} $ are the
random initial fan-in weights from the base weight initialization and
the projection is computed using <span
class="math inline"><em>V</em><sub><em>W</em><sub><em>l</em></sub></sub><sup>′</sup></span>
, comprised of orthogonal vectors of the kernel.It is named
<strong><em>NORTH-Weight</em></strong></p></li>
<li><p><strong>Gradient based</strong> : For this initialization, a
reimplementation of existing neurogenesis algorithm is used like NeST
from <a href="https://arxiv.org/abs/1711.02017">Dai et al. (2019)</a>,
<a href="https://arxiv.org/abs/2102.08574">Firefly from Wu et al.
(2020)</a>, and <a href="https://arxiv.org/abs/2201.05125">GradMax from
Evci et al. (2022)</a></p></li>
</ol>
</div>
<div class="cell markdown">
<p><strong>Quick recapitulatif of all the trigger and
initialization</strong></p>
<table>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Trigger</th>
<th>Initialization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NORTH-Select</td>
<td><span
class="math inline"><em>T</em><sub><em>a</em><em>c</em><em>t</em></sub></span></td>
<td>Select</td>
</tr>
<tr class="even">
<td>NORTH-Pre</td>
<td><span
class="math inline"><em>T</em><sub><em>a</em><em>c</em><em>t</em></sub></span></td>
<td>Pre-activation</td>
</tr>
<tr class="odd">
<td>NORTH-Random</td>
<td><span
class="math inline"><em>T</em><sub><em>a</em><em>c</em><em>t</em></sub></span></td>
<td>RandomInit</td>
</tr>
<tr class="even">
<td>NORTH-Weight</td>
<td><span
class="math inline"><em>T</em><sub><em>w</em><em>e</em><em>i</em><em>g</em><em>h</em><em>t</em></sub></span></td>
<td>Weight</td>
</tr>
<tr class="odd">
<td>GradMax</td>
<td><span
class="math inline"><em>T</em><sub><em>g</em><em>r</em><em>a</em><em>d</em></sub></span></td>
<td>GradMax</td>
</tr>
<tr class="even">
<td>Firefly</td>
<td><span
class="math inline"><em>T</em><sub><em>g</em><em>r</em><em>a</em><em>d</em></sub></span></td>
<td>Firefly</td>
</tr>
<tr class="odd">
<td>NeST</td>
<td><span
class="math inline"><em>T</em><sub><em>g</em><em>r</em><em>a</em><em>d</em></sub></span></td>
<td>NeST</td>
</tr>
</tbody>
</table>
</div>
<section id="experiments" class="cell markdown">
<h2>Experiments</h2>
</section>
<div class="cell markdown">
<p>We are now going to study the impact of the different trigger and
initialization methods detailed in Table 1 over a variety of tasks, with
dynamic schedules to study triggers and independently studying the
importance of initialization by also using fixed schedules. We are also
going to compare with static networks of various sizes to understand the
relative performance of networks grown with neurogenesis. Specifically,
we focus on MLPs on MNIST.</p>
</div>
<div class="cell markdown">
<p>In our investigation of neurogenesis within dense MLP layers for
MNIST classification, we utilize networks with vectorized image inputs,
two hidden layers that grow via neurogenesis, and ten output neurons as
we just saw before. We explore dynamic strategies and isolate
initializations in preset growth schedules. Test accuracy and training
time are compared against final network size or schedule, depicted in
Figure 1, while Figure 2 illustrates layer widths over training for
dynamic schedules.</p>
<p>To better understand the impact of trigger versus initialization
methods, we compare different initialization approaches using fixed
neurogenesis schedules. We employ two predefined schedules: Linear and
Batched.</p>
<ul>
<li>the <strong>Linear</strong> schedule adds one neuron per gradient
step.</li>
<li>The <strong>Batched</strong> schedule adds batches of neurons after
each gradient step.</li>
</ul>
<p>Both schedules are characterized by initial and final layer widths.
We allow neuron growth for the first 75% of epochs.</p>
<p>The <strong>Linear</strong> schedule results in frequent, small
growth events as neurons are added one at a time. The
<strong>Batched</strong> schedule, on the other hand, has fewer, larger
growth events as neurons are added in regular intervals. We also compare
these schedules with a Medium Static network, which begins training with
the final size of the growing networks, containing 256 neurons per
hidden layer.</p>
<figure>
  <center><img src="img\training_time.png" width="700px" ></center>
  <figcaption style="text-align:center;">Figure 1</figcaption>
</figure>
<figure>
  <center><img src="img\figure2.png" width="700px" ></center>
  <figcaption style="text-align:center;">Figure 2</figcaption>
</figure>
<p>Dynamic neurogenesis reveals that NORTH* orthogonality-based methods
dominate the Pareto front of accuracy versus network size (the Pareto
front of accuracy represents the trade-off between achieving higher
accuracy and minimizing network size or complexity). These methods
leverage activation and weight orthogonality triggers, outperforming
gradient-based approaches like GradMax and NeST, despite the latter's
faster CPU implementation due to optimized deep learning libraries for
gradient descent. The triggers effectively converge toward smaller
networks, responding to questions of "When" and "How many" neurons to
add. NORTH* methods tend to converge toward networks with larger first
layers and smaller second layers, resembling patterns in hand-designed
networks. Preset schedules show that multiple NORTH* strategies
outperform static networks of similar final architectures, highlighting
the benefits of dynamically growing networks during learning. However,
NORTH-Pre slightly underperformed compared to less-informed approaches,
such as NORTH-Select and NORTH-Random, indicating the importance of
trigger selection in neurogenesis strategies.</p>
</div>
<section id="conclusion" class="cell markdown">
<h2>Conclusion</h2>
</section>
<div class="cell markdown">
<p>NORTH* strategies enable the development of efficient networks that
achieve comparable performance to static architectures, and in some
cases even outperform them. Utilizing growth with informed triggers
leads to efficient networks as redundant neurons are less likely to be
created during the small network initialization and are not added by
NORTH* neurogenesis. Activation and weight orthogonality have been found
to be useful triggers for dynamic neurogenesis, along with weight
initializations of new neurons. These methods implicitly use gradient
information via network updates, thus potentially extending to
unsupervised and semi-supervised contexts where gradient information may
be unreliable. While neurogenesis triggers can be computationally
costly, NORTH* strategies have shown greater efficiency compared to
gradient-based methods in GPU experiments.</p>
</div>
<div class="cell markdown">
<p>We present the NORTH* suite of neurogenesis strategies, comprised of
triggers that use orthogonality metrics of either activations or weights
to determine when, where, and how many neurons to add and informed
initializations that optimize the metrics. NORTH* strategies achieve
dynamic neurogenesis, growing effective networks that converge in size
and can outperform baseline static networks in compactness or
performance. The orthogonality-based neurogenesis methods in NORTH*
could be further used to grow networks in a variety of settings, such as
continual learning, shifting distributions, or combined synergistically
with other structural learning methods. Neurogenesis can grow network
architectures that dynamically respond to learning.</p>
</div>
<section id="references" class="cell markdown">
<h1>References</h1>
</section>
<div class="cell markdown">
<p>All inspired by <a href="https://arxiv.org/abs/2202.08539">When,
where, and how to add new neurons to ANNs</a></p>
</div>
<div class="cell markdown">

</div>
</body>
</html>
